# -*- coding: utf-8 -*-
"""concrete_composite_strength_prediction_final[1].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ppcq-vCE9d6T8lyEEgeQXlSVoAkWj4o3

# MODELING OF STRENGTH OF HIGH-PERFORMANCE CONCRETE USING *ARTIFICIAL NEURAL NETWORKS*
"""

# import Data Manipulation library
import pandas as pd
import numpy as np

#Import Data visualizatio library
import matplotlib.pyplot as plt
import seaborn as sns

#Import filter warning library
import warnings
warnings.filterwarnings('ignore')

#Import scikit Learn library
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, FunctionTransformer

import scipy.stats as stats

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn import set_config
set_config(display='diagram')

# import Deep Learning Library
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense, Dropout
# import keras_tuner as kt

# Import Dataset Using Pandas Function

url = "https://raw.githubusercontent.com/jadhavgaurav/cement-composite-strength-prediction/refs/heads/main/concrete_data.csv"

df = pd.read_csv(url)

# Outlier Detection using IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Detecting outliers
outliers = ((df < lower_bound) | (df > upper_bound)).sum()

# Display number of outliers per feature
print("Number of outliers per feature:\n", outliers)

# import pandas as pd

# def cap_outliers_iqr(df, columns):
#     """
#     Caps outliers based on the IQR method for specified columns.

#     Parameters:
#         df (pd.DataFrame): The input DataFrame.
#         columns (list): List of column names to check for outliers.

#     Returns:
#         pd.DataFrame: DataFrame with outliers capped.
#     """
#     df_capped = df.copy()
#     for col in columns:
#         Q1 = df[col].quantile(0.25)  # First Quartile (25th percentile)
#         Q3 = df[col].quantile(0.75)  # Third Quartile (75th percentile)
#         IQR = Q3 - Q1                # Interquartile Range
#         lower_bound = Q1 - 1.5 * IQR  # Lower bound
#         upper_bound = Q3 + 1.5 * IQR  # Upper bound

#         # Cap values beyond the boundaries
#         df_capped[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

#     return df_capped

# # Cap outliers in specific columns
# columns_to_cap = ['strength_age_ratio', 'age', 'water_cement_ratio']  # Replace with actual column names
# df_capped = cap_outliers_iqr(df, columns_to_cap)

# # Display number of rows before and after capping
# print(f"Original dataset size: {df.shape[0]}")
# print(f"Capped dataset size: {df_capped.shape[0]} (No rows removed, only values capped)")

def remove_outliers_iqr(df, columns):
    """
    Removes outliers based on IQR method for specified columns.

    Parameters:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of column names to check for outliers.

    Returns:
        pd.DataFrame: DataFrame with outliers removed.
    """
    df_clean = df.copy()
    for col in columns:
        Q1 = df[col].quantile(0.25)  # First Quartile (25th percentile)
        Q3 = df[col].quantile(0.75)  # Third Quartile (75th percentile)
        IQR = Q3 - Q1                # Interquartile Range
        lower_bound = Q1 - 1.5 * IQR  # Lower bound
        upper_bound = Q3 + 1.5 * IQR  # Upper bound

        # Filter out outliers
        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]

    return df_clean


# Remove outliers from specific columns
columns_to_check = ['strength_age_ratio', 'age', 'water_cement_ratio']  # Replace with actual column names , o - 'water_cement_ratio'
df_cleaned = remove_outliers_iqr(df, columns_to_check)

# Display number of rows before and after removing outliers
print(f"Original dataset size: {df.shape[0]}")
print(f"Cleaned dataset size: {df_cleaned.shape[0]}")


# Split Data into X and y
X = df_cleaned.drop(columns = ['concrete_compressive_strength']) # Independent Features
y = df_cleaned['concrete_compressive_strength'] # target Variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape, X.shape, y_train.shape, y.shape

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# X_train.max(), X_train.min()

# X_train = pd.DataFrame(X_train, columns=X.columns)
# plt.figure(figsize=(16, 12))
# for i, col in enumerate(X_train.columns):
#     plt.subplot(4, 3, i+1)
#     sns.histplot(df[col], kde=True, color='blue')
#     plt.title(f"Distribution of {col}")
# plt.tight_layout()
# plt.show()

# import RandomForest
from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor()
model_rf.fit(X_train, y_train)

kf = 5

# Cross-validation scores on Training Data
train_r2_score = np.mean(cross_val_score(model_rf, X_train, y_train, cv=kf, scoring='r2'))
train_mae  = np.mean(cross_val_score(model_rf, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')) * -1
train_mse = np.mean(cross_val_score(model_rf, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')) * -1
train_rmse = np.sqrt(train_mse)

# Evaluate on Test Data
y_pred_test = model_rf.predict(X_test)
test_r2_score = r2_score(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)
test_mse = mean_squared_error(y_test, y_pred_test)
test_rmse = np.sqrt(test_mse)

# Print Evaluation Metrics
print('Evaluation for Random Forest:')
print('Train R2 Score  :', round(train_r2_score, 3))
print('Test R2 Score   :', round(test_r2_score, 3))
print('Train MAE       :', round(train_mae, 3))
print('Test MAE        :', round(test_mae, 3))
print('Train MSE       :', round(train_mse, 3))
print('Test MSE        :', round(test_mse, 3))
print('Train RMSE      :', round(train_rmse, 3))
print('Test RMSE       :', round(test_rmse, 3))

residuals = y_test - y_pred_test
sns.distplot(residuals)
plt.axvline(0,color = 'red')
plt.title('Residual Distribution for Random Forest')
plt.show()

import scipy.stats as stats


"""# Artifical Neural Network Model training"""

from tensorflow.keras.callbacks import ReduceLROnPlateau


# ANN Model
model_ann = keras.Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),  # Prevents overfitting
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.3),
    # Dense(16, activation='relu'),
    # Dropout(0.2),
    Dense(1)  # Output Layer
])

# # Define Adaptive Learning Rate Callback
# reduce_lr = ReduceLROnPlateau(
#     monitor='val_loss',  # Monitor validation loss
#     factor=0.5,  # Reduce LR by 50%
#     patience=5,  # Wait for 5 epochs before reducing LR
#     min_lr=1e-5  # Set a lower bound for LR
# )

# Compile the model
model_ann.compile(optimizer='adam', loss='mse', metrics=['mae'])


# Train the Model
history = model_ann.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), verbose=1)

model_ann.summary()


y_pred_train = model_ann.predict(X_train)
y_pred_test = model_ann.predict(X_test)

"""## ANN Model Evaluation"""

# Evaluate on Train Data
y_pred_train = model_ann.predict(X_train)
train_r2_score = r2_score(y_train, y_pred_train)
train_mae = mean_absolute_error(y_train, y_pred_train)
train_mse = mean_squared_error(y_train, y_pred_train)
train_rmse = np.sqrt(train_mse)

# Evaluate on Test Data
y_pred_test = model_ann.predict(X_test)
test_r2_score = r2_score(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)
test_mse = mean_squared_error(y_test, y_pred_test)
test_rmse = np.sqrt(test_mse)

# Print Evaluation Metrics
print('Evaluation for Artificial Neural Network:')
print('Train R2 Score  :', round(train_r2_score, 3))
print('Test R2 Score   :', round(test_r2_score, 3))
print('Train MAE       :', round(train_mae, 3))
print('Test MAE        :', round(test_mae, 3))
print('Train MSE       :', round(train_mse, 3))
print('Test MSE        :', round(test_mse, 3))
print('Train RMSE      :', round(train_rmse, 3))
print('Test RMSE       :', round(test_rmse, 3))

residuals = y_test - y_pred_test.flatten()
sns.distplot(residuals)
plt.axvline(0,color = 'red')
plt.title('Residual Distribution')
plt.show()


import joblib

# Save only the fitted scaler
joblib.dump(scaler, "scaler.pkl")
print("Scaler saved as scaler.pkl")

# Save the trained model
model_ann.save("final_model.keras")
print("Trained model saved as final_model.keras")

