# -*- coding: utf-8 -*-
"""concrete_composite_strength_prediction_final[1].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ppcq-vCE9d6T8lyEEgeQXlSVoAkWj4o3

# MODELING OF STRENGTH OF HIGH-PERFORMANCE CONCRETE USING *ARTIFICIAL NEURAL NETWORKS*
"""

# import Data Manipulation library
import pandas as pd
import numpy as np

#Import Data visualizatio library
import matplotlib.pyplot as plt
import seaborn as sns

#Import filter warning library
import warnings
warnings.filterwarnings('ignore')

#Import scikit Learn library
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, FunctionTransformer

import scipy.stats as stats

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn import set_config
set_config(display='diagram')

# import Deep Learning Library
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense, Dropout
# import keras_tuner as kt
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import load_model

"""# Concrete Compressive Strength Dataset

## Overview
- **Source**: [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)
- **Dataset Type**: Regression
- **Number of Instances**: 1030
- **Number of Attributes**: 9 input variables + 1 target variable
- **Domain**: Civil Engineering, Material Science
- **Objective**: Predict the compressive strength of concrete based on its composition and age.

## Attribute Information
| Column | Description | Unit |
|---------|------------------------------|--------------|
| Cement | Cement component | kg/m¬≥ |
| Blast Furnace Slag | Blast furnace slag component | kg/m¬≥ |
| Fly Ash | Fly ash component | kg/m¬≥ |
| Water | Water component | kg/m¬≥ |
| Superplasticizer | Superplasticizer additive | kg/m¬≥ |
| Coarse Aggregate | Coarse aggregate component | kg/m¬≥ |
| Fine Aggregate | Fine aggregate component | kg/m¬≥ |
| Age | Age of concrete samples | Days |
| Compressive Strength | Target variable - Strength of concrete | MPa |

## Dataset Characteristics
- The dataset contains **continuous numerical features**.
- The target variable (compressive strength) is influenced by **composition and age**.
- No categorical features.

## Applications
- Predicting the strength of concrete for **construction quality control**.
- Understanding the impact of different components on **material durability**.
- Optimizing concrete composition for **stronger and cost-effective construction**.

## Source & Citation
Yeh, I-C. "Modeling of Strength of High-Performance Concrete Using Artificial Neural Networks." *Cement and Concrete Research*, Vol. 28, No. 12, pp. 1797-1808, 1998.

---
**Next Steps:**
- Load the dataset and check for missing values.
- Perform **EDA** (Exploratory Data Analysis) to understand feature distributions.
- Build regression models to predict compressive strength.

"""

# Import Dataset Using Pandas Function

url = 'https://raw.githubusercontent.com/jadhavgaurav/cement-composite-strength-prediction/refs/heads/main/concrete_data.csv'

df = pd.read_csv(url)
print(df.sample(frac = 1)) # Shuffle Dataset

print(df.columns)

print(df.columns = df.columns.str.strip())

# Checking Data information and Missing Values if any...

print(df.info())

# Checking Descriptive Stattistics
print(df.describe())

"""## Based on above information, we find that the dataset is non normal distributed"""

# Univariate Analysis (Custom Function)

from collections import OrdeprintredDict

stats = []
for i in df.columns:
    numerical_stats = OrderedDict({
        'Feature': i,
        'Maximum' : df[i].max(),
        'Minimum' : df[i].min(),
        'Mean' : df[i].mean(),
        'Median' : df[i].median(),
        '25%': df[i].quantile(0.25),
        '75%': df[i].quantile(0.75),
        'Standard Deviation': df[i].std(),
        'Variance': df[i].var(),
        'Skewness': df[i].skew(),
        'Kurtosis': df[i].kurt(),
        'IQR' : df[i].quantile(0.75) - df[i].quantile(0.25)
    })
    stats.append(numerical_stats)
report = pd.DataFrame(stats)
print(report)

"""## Univariate Analysis
### 1. Histograms & KDE Plots
"""

plt.figure(figsize=(16, 12))
for i, col in enumerate(df.columns):
    plt.subplot(3, 3, i+1)
    sns.histplot(df[col], kde=True, color='blue')
    plt.title(f"Distribution of {col}")
plt.tight_layout()
plt.show()

"""- Cement, water, and aggregates follow near-normal distributions, meaning these - materials have consistent usage across different mixes.
- Blast furnace slag, fly ash, and superplasticizer are highly skewed, indicating - they are often absent or used in small amounts.
- Age distribution suggests that most concrete samples are tested at an early - stage, while long-term strength is measured less frequently.
- The strength distribution shows that most concrete samples achieve 30-50 MPa - compressive strength, which is a standard range for construction applications.

### 2. Box Plots for Outlier Detection
"""

# Boxplot for detecting outliers in the dataset
plt.figure(figsize=(14, 10))
plot = 0
for i in df.columns:
    plot += 1
    plt.subplot(4, 3, plot)
    sns.boxenplot(df[i], color='violet')
    plt.tight_layout()
plt.show()

"""# Bivariate Analysis
#    4.1. Correlation Matrix
"""

df.corr()['concrete_compressive_strength']

corr_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Correlation Matrix")
plt.show()

"""1Ô∏è‚É£ Cement is the most critical factor (+0.50 correlation) ‚Äì Higher cement content significantly increases concrete compressive strength.

2Ô∏è‚É£ Superplasticizer improves strength (+0.37 correlation) ‚Äì It enhances workability while reducing water, leading to stronger concrete.

3Ô∏è‚É£ Age matters (+0.33 correlation) ‚Äì As concrete cures over time, its strength increases.

4Ô∏è‚É£ Water negatively affects strength (-0.29 correlation) ‚Äì Excess water weakens concrete, reducing durability.

5Ô∏è‚É£ Aggregates (fine & coarse) have minimal direct impact (~-0.16 correlation) ‚Äì While essential for structure, their contribution to compressive strength is not significant.

6Ô∏è‚É£ Fly ash and blast furnace slag have weak correlations (~0.13 to -0.11) ‚Äì These materials don‚Äôt drastically impact strength but may influence durability and workability.

7Ô∏è‚É£ Water and superplasticizer are strongly negatively correlated (-0.66) ‚Äì More superplasticizer means less water is needed, leading to better strength.

### 2. Pair Plot
"""

sns.pairplot(df)

"""1Ô∏è‚É£ **Strong Positive Correlation:** <br>
Cement vs. Compressive Strength ‚Üí As cement content increases, strength significantly improves. <br>
Age vs. Compressive Strength ‚Üí Older concrete has higher strength due to curing effects.<br>

2Ô∏è‚É£ **Negative Correlation:** <br>
Water vs. Compressive Strength ‚Üí Higher water content weakens concrete, leading to lower strength. <br>
Water vs. Superplasticizer ‚Üí More superplasticizer reduces water requirement, improving workability.<br>

3Ô∏è‚É£ **Weak or No Significant Trends:** <br>
Coarse & Fine Aggregate vs. Compressive Strength ‚Üí No clear impact; aggregates provide structure but don‚Äôt directly boost strength. <br>
Fly Ash & Blast Furnace Slag vs. Strength ‚Üí Some contribution but not a strong determinant.<br>

4Ô∏è‚É£ **Distribution Insights:** <br>
Some variables (like water and superplasticizer) have skewed distributions, indicating possible outliers or non-uniform data distribution. <br>
Compressive strength has a right-skewed distribution, meaning most values are lower, with fewer high-strength samples.

- Maximize cement & curing time for better strength.
- Reduce water & optimize superplasticizer for better workability and durability.
- Reevaluate the role of aggregates in mix design.

## Multivariate Analysis / Deeper Outlier Inspection
### 1. Using a combination of features
### For instance, we could see if certain features combined
"""

from mpl_toolkits.mplot3d import Axes3D
target_col = 'concrete_compressive_strength'

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Choose three features for demonstration
x_feat = 'cement'
y_feat = 'water'
z_feat = 'age'

scatter = ax.scatter(df[x_feat],
                     df[y_feat],
                     df[z_feat],
                     c=df[target_col],
                     cmap='viridis',
                     alpha=0.7)

ax.set_xlabel(x_feat)
ax.set_ylabel(y_feat)
ax.set_zlabel(z_feat)
cbar = fig.colorbar(scatter, ax=ax, shrink=0.5)
cbar.set_label(target_col)
plt.title("3D Scatter: Cement vs Water vs Age, colored by Strength")
plt.show()

"""1Ô∏è‚É£ Higher Cement = Higher Strength ‚Üí More cement content generally results in stronger concrete. <br>
2Ô∏è‚É£ Higher Age = Higher Strength ‚Üí Older concrete (higher curing time) shows greater compressive strength. <br>
3Ô∏è‚É£ Higher Water = Lower Strength ‚Üí Excess water reduces strength, confirming the water-to-cement ratio's impact.

üìå Conclusions for Optimization:

- Increase cement while keeping water in check for higher strength.
- Allow longer curing time to enhance strength.
- Balance water-to-cement ratio to prevent strength reduction.
"""

"""# Feature Engineering"""

# 1. Water-to-Cement Ratio (w/c Ratio)
# The ratio of water to cement significantly influences concrete strength.

df['water_cement_ratio'] = df['water'] / df['cement']

df['strength_age_ratio'] = df['concrete_compressive_strength'] / df['age']

new_features = ['water_cement_ratio', 'strength_age_ratio']

"""## Variance Inflation Factor"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm  # Importing statsmodels

X = df.drop(columns='concrete_compressive_strength')  # Selecting only independent variables

# Add a small constant to avoid division errors
X = X + 1e-10

# Compute VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF values
print(vif_data)

"""### We will drop the features with High VIF(variance-inflation-factor) as they impact model performance"""

df.drop(columns=['water', 'coarse_aggregate', 'fine_aggregate'], inplace=True)

from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm  # Importing statsmodels

X = df.drop(columns='concrete_compressive_strength')  # Selecting only independent variables

# Add a small constant to avoid division errors
X = X + 1e-10

# Compute VIF for each feature
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF values
print(vif_data)

"""VIF for all the features is less than 10, So the features will lead to good model performance"""

corr_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Correlation Matrix")
plt.show()

"""1. Correlation Analysis:
- **Strong Positive Correlations:**

- Cement vs. Concrete Compressive Strength (0.50): More cement leads to stronger concrete.
- Superplasticizer vs. Concrete Compressive Strength (0.37): Superplasticizers improve concrete strength.
- Age vs. Concrete Compressive Strength (0.33): Strength increases with curing time.

- **Strong Negative Correlations:**

- Water-Cement Ratio vs. Concrete Compressive Strength (-0.50): More water weakens the concrete.
- Water-Cement Ratio vs. Cement (-0.88): Higher cement reduces the water-cement ratio, indicating an inverse relationship.
- Strength-Age Ratio vs. Age (-0.40): As concrete ages, the ratio of strength gain per unit time decreases.

# Handling Outliers
"""

# Outlier Detection using IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Detecting outliers
outliers = ((df < lower_bound) | (df > upper_bound)).sum()

# Display number of outliers per feature
print("Number of outliers per feature:\n", outliers)

# import pandas as pd

# def cap_outliers_iqr(df, columns):
#     """
#     Caps outliers based on the IQR method for specified columns.

#     Parameters:
#         df (pd.DataFrame): The input DataFrame.
#         columns (list): List of column names to check for outliers.

#     Returns:
#         pd.DataFrame: DataFrame with outliers capped.
#     """
#     df_capped = df.copy()
#     for col in columns:
#         Q1 = df[col].quantile(0.25)  # First Quartile (25th percentile)
#         Q3 = df[col].quantile(0.75)  # Third Quartile (75th percentile)
#         IQR = Q3 - Q1                # Interquartile Range
#         lower_bound = Q1 - 1.5 * IQR  # Lower bound
#         upper_bound = Q3 + 1.5 * IQR  # Upper bound

#         # Cap values beyond the boundaries
#         df_capped[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

#     return df_capped

# # Cap outliers in specific columns
# columns_to_cap = ['strength_age_ratio', 'age', 'water_cement_ratio']  # Replace with actual column names
# df_capped = cap_outliers_iqr(df, columns_to_cap)

# # Display number of rows before and after capping
# print(f"Original dataset size: {df.shape[0]}")
# print(f"Capped dataset size: {df_capped.shape[0]} (No rows removed, only values capped)")

def remove_outliers_iqr(df, columns):
    """
    Removes outliers based on IQR method for specified columns.

    Parameters:
        df (pd.DataFrame): The input DataFrame.
        columns (list): List of column names to check for outliers.

    Returns:
        pd.DataFrame: DataFrame with outliers removed.
    """
    df_clean = df.copy()
    for col in columns:
        Q1 = df[col].quantile(0.25)  # First Quartile (25th percentile)
        Q3 = df[col].quantile(0.75)  # Third Quartile (75th percentile)
        IQR = Q3 - Q1                # Interquartile Range
        lower_bound = Q1 - 1.5 * IQR  # Lower bound
        upper_bound = Q3 + 1.5 * IQR  # Upper bound

        # Filter out outliers
        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]

    return df_clean


# Remove outliers from specific columns
columns_to_check = ['strength_age_ratio', 'age', 'water_cement_ratio']  # Replace with actual column names , o - 'water_cement_ratio'
df_cleaned = remove_outliers_iqr(df, columns_to_check)

# Display number of rows before and after removing outliers
print(f"Original dataset size: {df.shape[0]}")
print(f"Cleaned dataset size: {df_cleaned.shape[0]}")

# Checking Column Names
print(df_cleaned.columns)

# Split Data into X and y
X = df_cleaned.drop(columns = ['concrete_compressive_strength']) # Independent Features
y = df_cleaned['concrete_compressive_strength'] # target Variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train.shape, X.shape, y_train.shape, y.shape

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# X_train.max(), X_train.min()

# X_train = pd.DataFrame(X_train, columns=X.columns)
# plt.figure(figsize=(16, 12))
# for i, col in enumerate(X_train.columns):
#     plt.subplot(4, 3, i+1)
#     sns.histplot(df[col], kde=True, color='blue')
#     plt.title(f"Distribution of {col}")
# plt.tight_layout()
# plt.show()

# import RandomForest
from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor()
model_rf.fit(X_train, y_train)

kf = 5

# Cross-validation scores on Training Data
train_r2_score = np.mean(cross_val_score(model_rf, X_train, y_train, cv=kf, scoring='r2'))
train_mae  = np.mean(cross_val_score(model_rf, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')) * -1
train_mse = np.mean(cross_val_score(model_rf, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')) * -1
train_rmse = np.sqrt(train_mse)

# Evaluate on Test Data
y_pred_test = model_rf.predict(X_test)
test_r2_score = r2_score(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)
test_mse = mean_squared_error(y_test, y_pred_test)
test_rmse = np.sqrt(test_mse)

# Print Evaluation Metrics
print('Evaluation for Random Forest:')
print('Train R2 Score  :', round(train_r2_score, 3))
print('Test R2 Score   :', round(test_r2_score, 3))
print('Train MAE       :', round(train_mae, 3))
print('Test MAE        :', round(test_mae, 3))
print('Train MSE       :', round(train_mse, 3))
print('Test MSE        :', round(test_mse, 3))
print('Train RMSE      :', round(train_rmse, 3))
print('Test RMSE       :', round(test_rmse, 3))

residuals = y_test - y_pred_test
sns.distplot(residuals)
plt.axvline(0,color = 'red')
plt.title('Residual Distribution for Random Forest')
plt.show()

import scipy.stats as stats

# Q-Q Plot for Normality Check
plt.figure(figsize=(6,5))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("Q-Q Plot of Residuals - Random Forest")
plt.show()

# y_pred_test.shape, residuals.shape

# Residuals vs. Fitted Plot
plt.figure(figsize=(6,5))
plt.scatter(y_pred_test, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Fitted Values (Predicted)")
plt.ylabel("Residuals")
plt.title("Residuals vs Fitted Values - Random Forest")
plt.show()

"""# Artifical Neural Network Model training"""

from tensorflow.keras.callbacks import ReduceLROnPlateau


# ANN Model
model_ann = keras.Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),  # Prevents overfitting
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.3),
    # Dense(16, activation='relu'),
    # Dropout(0.2),
    Dense(1)  # Output Layer
])

# # Define Adaptive Learning Rate Callback
# reduce_lr = ReduceLROnPlateau(
#     monitor='val_loss',  # Monitor validation loss
#     factor=0.5,  # Reduce LR by 50%
#     patience=5,  # Wait for 5 epochs before reducing LR
#     min_lr=1e-5  # Set a lower bound for LR
# )

# Compile the model
model_ann.compile(optimizer='adam', loss='mse', metrics=['mae'])


# Train the Model
history = model_ann.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), verbose=1)

model_ann.summary()

"""## Model Training Curves"""

hist = model_ann.history.history
hist = pd.DataFrame(hist)
hist.plot()
plt.title('Model Metrics')
plt.show()

y_pred_train = model_ann.predict(X_train)
y_pred_test = model_ann.predict(X_test)

"""## ANN Model Evaluation"""

# Evaluate on Train Data
y_pred_train = model_ann.predict(X_train)
train_r2_score = r2_score(y_train, y_pred_train)
train_mae = mean_absolute_error(y_train, y_pred_train)
train_mse = mean_squared_error(y_train, y_pred_train)
train_rmse = np.sqrt(train_mse)

# Evaluate on Test Data
y_pred_test = model_ann.predict(X_test)
test_r2_score = r2_score(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)
test_mse = mean_squared_error(y_test, y_pred_test)
test_rmse = np.sqrt(test_mse)

# Print Evaluation Metrics
print('Evaluation for Artificial Neural Network:')
print('Train R2 Score  :', round(train_r2_score, 3))
print('Test R2 Score   :', round(test_r2_score, 3))
print('Train MAE       :', round(train_mae, 3))
print('Test MAE        :', round(test_mae, 3))
print('Train MSE       :', round(train_mse, 3))
print('Test MSE        :', round(test_mse, 3))
print('Train RMSE      :', round(train_rmse, 3))
print('Test RMSE       :', round(test_rmse, 3))

residuals = y_test - y_pred_test.flatten()
sns.distplot(residuals)
plt.axvline(0,color = 'red')
plt.title('Residual Distribution')
plt.show()

"""- Normally Distributed Residuals - The histogram and KDE plot show a bell-shaped curve, indicating that residuals are approximately normally distributed.
- Centered Around Zero - The red vertical line at zero suggests that the model has low bias, as the residuals are symmetrically distributed around zero.
- No Major Skewness - The distribution is balanced, meaning no major over-prediction or under-prediction trends.
- Low Residual Variability - Most residuals are within a narrow range, indicating good model performance with minimal errors.
- **Conclusion: The ANN model is well-fitted, showing low bias and normally distributed residuals, making it reliable for predictions. üöÄ**
"""

# Q-Q Plot for Normality Check
plt.figure(figsize=(6,5))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("Q-Q Plot of Residuals")
plt.show()

# Residuals vs. Fitted Plot
plt.figure(figsize=(6,5))
plt.scatter(y_pred_test, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Fitted Values (Predicted)")
plt.ylabel("Residuals")
plt.title("Residuals vs Fitted Values - ANN")
plt.show()

"""- No Clear Pattern - Residuals are randomly scattered around the red dashed line (zero), suggesting that the model captures the relationship well and there is no major systematic error.
- Homoscedasticity (Constant Variance) - The spread of residuals appears relatively uniform across different predicted values, indicating that heteroscedasticity is not a major issue.
- Some Outliers - A few residuals are noticeably large, suggesting some instances where predictions deviate significantly from actual values.
- Good Model Fit - Since there is no strong trend, the ANN model appears to be making unbiased predictions.
- **Conclusion: The ANN model performs well with no major bias or heteroscedasticity issues, but further fine-tuning may help address the few large residuals. üöÄ**

# Save Trained model and preprocessor
"""

import joblib

# Save only the fitted scaler
joblib.dump(scaler, "scaler.pkl")
print("Scaler saved as scaler.pkl")

# Save the trained model
model_ann.save("final_model.keras")
print("Trained model saved as final_model.keras")

